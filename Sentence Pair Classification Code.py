# -*- coding: utf-8 -*-
"""Vasilescu_Andreea_512_Sentence_Pair_Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UhWtez2AzZdCQGSzYSxkv6zonPCd8kLM

# Converting text to data

Connect to google drive
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

"""Extract the content of the file in an extraction directory"""

import zipfile
import os

zip_file_path = '/content/gdrive/MyDrive/sentence-pair-classification-pml-2023.zip'

extracted_directory = '/content/gdrive/MyDrive/sentence-pair-classification-pml-2023'

os.makedirs(extracted_directory, exist_ok=True)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_directory)

extracted_files = os.listdir(extracted_directory)
print("Extracted files:", extracted_files)

import json
import pandas as pd

def load_json_to_df(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return pd.DataFrame(data)

# training data

train_file_path = '/content/gdrive/MyDrive/sentence-pair-classification-pml-2023/train.json'
train_data =  load_json_to_df(train_file_path)

# testing data
test_file_path = '/content/gdrive/MyDrive/sentence-pair-classification-pml-2023/test.json'
test_data = load_json_to_df(test_file_path)


# validation data
validation_file_path = '/content/gdrive/MyDrive/sentence-pair-classification-pml-2023/validation.json'
validation_data = validation_df = load_json_to_df(validation_file_path)

train_data

"""#Preprocessing



*   remove stop words
*   lowercasing
*   lemmatizatize
*   remove stop words
*   tokenize


"""

import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def preprocess(text):

  text = text.lower()

  text = ''.join(([char for char in text if char not in string.punctuation and not char.isdigit()]))

  tokens = word_tokenize(text)

  stop_words = set(stopwords.words('romanian'))
  tokens = [word for word in tokens if word not in stop_words]

  lemmatizer = WordNetLemmatizer()
  tokens = [lemmatizer.lemmatize(word) for word in tokens]

  preprocessed_text = ' '.join(tokens)

  return preprocessed_text

train_data['sentence1'] = train_data['sentence1'].apply(preprocess)
train_data['sentence2'] = train_data['sentence2'].apply(preprocess)

validation_data['sentence1'] = validation_data['sentence1'].apply(preprocess)
validation_data['sentence2'] = validation_data['sentence2'].apply(preprocess)

test_data['sentence1'] = test_data['sentence1'].apply(preprocess)
test_data['sentence2'] = test_data['sentence2'].apply(preprocess)

sentence1_train = train_data['sentence1']
sentence2_train = train_data['sentence2']
label_train = train_data['label']
guid_train = train_data['guid']

sentence1_valid = validation_data['sentence1']
sentence2_valid = validation_data['sentence2']
label_valid = validation_data['label']
guid_valid = validation_data['guid']

sentence1_test = test_data['sentence1']
sentence2_test = test_data['sentence2']
guid_test = test_data['guid']

sentence1_test

"""#Building a model

TfidfVectorizer is a feature extraction technique commonly used in natural language processing and information retrieval. The term "TF-IDF" stands for "Term Frequency-Inverse Document Frequency."

Here's a breakdown of how it works:

Term Frequency (TF): Measures the frequency of a term (word) in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document.

Inverse Document Frequency (IDF): Measures the importance of a term across a collection of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.

TF-IDF: The product of TF and IDF. It gives a higher weight to terms that are frequent in a specific document but not common across all documents. Terms that are common across all documents receive lower weights.

The TfidfVectorizer in scikit-learn is a convenient tool that combines the processes of tokenization, counting, and TF-IDF weighting. Given a collection of text documents, it transforms the text into a matrix of TF-IDF features.

## TF-IDF vectorizer

Combining sentences

X_train for training, X_valid for validation, X_test for testing

Applied Model SVC



1.   For basic configuration: On Kaggle I got 0.50,  the accuracy was 0,65, precision 0.49, recall 0.35
"""

import scipy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

vectorizer = TfidfVectorizer()

# vectorize training data

X_train = vectorizer.fit_transform(sentence1_train + ' ' +  sentence2_train)

X_valid = vectorizer.transform(sentence1_valid + ' ' +  sentence2_valid)
X_test = vectorizer.transform(sentence1_test + ' ' +  sentence2_test)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

svc_model = SVC()
svc_model.fit(X_train, label_train)

predictions_svc = svc_model.predict(X_valid)

"""Analyze the metrics for validation"""

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate accuracy, precision, recall, and F1 score
accuracy_svc = accuracy_score(label_valid, predictions_svc)
precision = precision_score(label_valid, predictions_svc, average='macro')
recall = recall_score(label_valid, predictions_svc, average='macro')
f1 = f1_score(label_valid, predictions_svc, average='macro')

print(f"SVC Validation Accuracy: {accuracy_svc}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""Testing data"""

predictions_test = svc_model.predict(X_test)

results_df = pd.DataFrame({'guid': guid_test, 'label': predictions_test})

results_df

"""Create a new df with labels for testing, save the data as csv"""

import pandas as pd

results_df = pd.DataFrame({'guid': guid_test, 'label': predictions_test})

output_path = '/content/gdrive/MyDrive/submission.csv'

results_df.to_csv(output_path, sep=',', index=False)

"""# Test SVC gridsearch"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import pandas as pd


param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}


svc = SVC()

grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='f1_macro', refit=True)

grid_search.fit(X_train, label_train)

results_df = pd.DataFrame(grid_search.cv_results_)

for index, row in results_df.iterrows():
    params = row['params']
    mean_f1_macro = row['mean_test_score']
    std_f1_macro = row['std_test_score']

    print(f"Parameters: {params}, Mean Macro F1 Score: {mean_f1_macro:.4f}, Std Macro F1 Score: {std_f1_macro:.4f}")

print("\nBest parameters: ", grid_search.best_params_)
print("Best Macro F1 score: ", grid_search.best_score_)

# Save the output to CSV to send it out

best_svc_model = grid_search.best_estimator_

predictions_svc_best = best_svc_model.predict(X_valid)

results_df_best_params_SVC = pd.DataFrame({'guid': guid_test, 'label': predictions_svc_best})

output_path = '/content/gdrive/MyDrive/GridSearchSVC.csv'

results_df.to_csv(output_path, sep=',', index=False)


f1_macro = f1_score(label_valid, predictions_svc, average='macro')
print("\nValidation Macro F1 score of the best model: ", f1_macro)

"""# XGBOOST

Generic -> 0.30

Test pe hyperparametrii

Cea mai buna acuratete pe mai multi hyperparametrii:

Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators':200}
Best XGBoost Validation Accuracy: 0.6348479895390651

"""

! pip install XGBoost

import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, label_train)

predictions_xgb = xgb_model.predict(X_valid)

accuracy_xgb = accuracy_score(label_valid, predictions_xgb)
precision_xgb = precision_score(label_valid, predictions_xgb, average='macro')
recall_xgb = recall_score(label_valid, predictions_xgb, average='macro')
f1_xgb = f1_score(label_valid, predictions_xgb, average='macro')


print(f"XGBoost Validation Accuracy: {accuracy_xgb}")
print(f"XGBoost Precision: {precision_xgb}")
print(f"XGBoost Recall: {recall_xgb}")
print(f"XGBoost F1 Score: {f1_xgb}")

predictions_test_xgb = xgb_model.predict(X_test)

results_df_xgb = pd.DataFrame({'guid': guid_test, 'label': predictions_test_xgb})

print("XGBoost Test Results:")
print(results_df_xgb)

import pandas as pd

results_df = pd.DataFrame({'guid': guid_test, 'label': predictions_test_xgb})

output_path = '/content/gdrive/MyDrive/submissionXGB.csv'

results_df.to_csv(output_path, sep=',', index=False)

"""# Test pe parametrii"""

from sklearn.model_selection import GridSearchCV

from sklearn.metrics import precision_score, recall_score, f1_score

param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
}

grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, label_train)

best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

best_xgb_model = grid_search.best_estimator_

predictions_xgb_best = best_xgb_model.predict(X_valid)

precision_xgb_best = precision_score(label_valid, predictions_xgb_best, average='macro')
recall_xgb_best = recall_score(label_valid, predictions_xgb_best, average='macro')
f1_xgb_best = f1_score(label_valid, predictions_xgb_best, average='macro')

print(f"XGB Validation Accuracy: {accuracy_xgb_best}")
print(f"Precision: {precision_xgb_best}")
print(f"Recall: {recall_xgb_best}")
print(f"F1 Score: {f1_xgb_best}")

"""# Random Forest

### test1
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1)

clf.fit(X_train, label_train)


y_pred = clf.predict(X_test)

import pandas as pd

results_df = pd.DataFrame({'guid': guid_test, 'label': y_pred})

output_path = '/content/gdrive/MyDrive/RF.csv'

results_df.to_csv(output_path, sep=',', index=False)

"""### test2

"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
import pandas as pd

clf = RandomForestClassifier(n_estimators=3000)

clf.fit(X_train, label_train)

y_pred_v2 = clf.predict(X_test)

results_df = pd.DataFrame({'guid': guid_test, 'label': y_pred_v2})
output_path = '/content/gdrive/MyDrive/RF2.csv'
results_df.to_csv(output_path, sep=',', index=False)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

clf = RandomForestClassifier()

grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

print("Best parameters: ", grid_search.best_params_)
print("Best accuracy: ", grid_search.best_score_)

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Test accuracy of the best model: ", accuracy)

"""### Things I have tried out, but didn't actually used

### checked best hyperparams sepparate
"""

import xgboost
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

best_hyperparameters = {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200}

xgb_model = xgb.XGBClassifier(**best_hyperparameters)
xgb_model.fit(X_train, label_train)

predictions_xgb = xgb_model.predict(X_valid)

accuracy_xgb = accuracy_score(label_valid, predictions_xgb)
precision_xgb = precision_score(label_valid, predictions_xgb, average='macro')
recall_xgb = recall_score(label_valid, predictions_xgb, average='macro')
f1_xgb = f1_score(label_valid, predictions_xgb, average='macro')

print(f"XGBoost Validation Accuracy: {accuracy_xgb}")
print(f"XGBoost Precision: {precision_xgb}")
print(f"XGBoost Recall: {recall_xgb}")
print(f"XGBoost F1 Score: {f1_xgb}")

best_hyperparameters = {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200}

best_predictions_test_xgb = xgb_model.predict(X_test)

results_df_xgb = pd.DataFrame({'guid': guid_test, 'label': best_predictions_test_xgb})

print("XGBoost Test Results:")
print("Best XGBoost Hyperparameters:", best_hyperparameters)
print(results_df_xgb)

print(best_predictions_test_xgb)

import pandas as pd

results_df = pd.DataFrame({'guid': guid_test, 'label': best_predictions_test_xgb})

output_path = '/content/gdrive/MyDrive/XGBhyperparam.csv'

results_df.to_csv(output_path, sep=',', index=False)

"""Test to see if I can save the model instead of running it all over again"""

print("Number of features in training data:", X_train_vec.shape[1])
print("Number of features in validation data:", X_valid_vec.shape[1])